# save_files.py
import os

files = {
    "apex_arc_engine/api/main.py": """
from fastapi import FastAPI
from api.endpoints.prediction import router as prediction_router
from utils.logger import setup_logger

logger = setup_logger(__name__)

app = FastAPI(title="Apex Arc Engine API")

app.include_router(prediction_router)

@app.on_event("startup")
async def startup_event():
    logger.info("Apex Arc Engine API started")
""",
    "apex_arc_engine/api/endpoints/prediction.py": """
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from datetime import date
from typing import List
from prediction.main import get_hr_predictions

router = APIRouter(prefix="/predictions", tags=["predictions"])

class PredictionRequest(BaseModel):
    prediction_date: date
    explain: bool = False

class PredictionResponse(BaseModel):
    player_id: str
    hr_probability: float
    quantum_entropy_score: float
    zeno_suppression_tag: float
    ghost_tier_risk: float
    sync_cluster_correlation: float
    explanation: dict = {}

@router.post("/hr", response_model=List[PredictionResponse])
async def predict_home_runs(request: PredictionRequest):
    try:
        predictions = get_hr_predictions(
            date=request.prediction_date,
            top_n=100,
            explain=request.explain
        )
        return [
            PredictionResponse(
                player_id=pred["player_id"],
                hr_probability=pred["hr_probability"],
                quantum_entropy_score=pred.get("quantum_entropy_score", 0.0),
                zeno_suppression_tag=pred.get("zeno_suppression_tag", 0.0),
                ghost_tier_risk=pred.get("ghost_tier_risk", 0.0),
                sync_cluster_correlation=pred.get("sync_cluster_correlation", 0.0),
                explanation=pred.get("explanation", {})
            )
            for pred in predictions
        ]
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail="Internal server error")
""",
    "apex_arc_engine/data_ingestion/real_time/statcast.py": """
import requests
import pandas as pd
from datetime import date
from utils.logger import setup_logger

logger = setup_logger(__name__)

def fetch_statcast_data(game_date: date) -> pd.DataFrame:
    try:
        url = f"https://api.statcast.example.com/data?date={game_date}"
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        data = response.json()
        df = pd.DataFrame(data["statcast"])
        required_columns = ["player_id", "exit_velocity", "launch_angle", "barrel_percentage"]
        if not all(col in df.columns for col in required_columns):
            raise ValueError("Missing required Statcast columns")
        logger.info(f"Fetched Statcast data for {game_date} with {len(df)} records")
        return df
    except requests.RequestException as e:
        logger.error(f"Failed to fetch Statcast data: {e}")
        raise
    except ValueError as e:
        logger.error(f"Data validation error: {e}")
        raise
""",
    "apex_arc_engine/data_ingestion/real_time/weather.py": """
import requests
import pandas as pd
from datetime import date
from utils.logger import setup_logger

logger = setup_logger(__name__)

def fetch_weather_data(game_date: date) -> pd.DataFrame:
    try:
        url = f"https://api.weather.example.com/data?date={game_date}"
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        data = response.json()
        df = pd.DataFrame(data["weather"])
        required_columns = ["stadium_id", "wind_speed", "wind_direction", "humidity"]
        if not all(col in df.columns for col in required_columns):
            raise ValueError("Missing required weather columns")
        logger.info(f"Fetched weather data for {game_date} with {len(df)} records")
        return df
    except requests.RequestException as e:
        logger.error(f"Failed to fetch weather data: {e}")
        return pd.DataFrame()
    except ValueError as e:
        logger.error(f"Data validation error: {e}")
        return pd.DataFrame()
""",
    "apex_arc_engine/feature_engineering/metrics.py": """
import pandas as pd
from utils.logger import setup_logger

logger = setup_logger(__name__)

def compute_metrics(data: pd.DataFrame) -> pd.DataFrame:
    try:
        required_columns = ["player_id", "exit_velocity", "launch_angle", "barrel_percentage"]
        if not all(col in data.columns for col in required_columns):
            raise ValueError("Missing required columns for feature engineering")
        features = data.copy()
        features["avg_exit_velocity"] = features["exit_velocity"].rolling(window=10).mean()
        features["barrel_rate"] = features["barrel_percentage"] / 100.0
        features["launch_angle_optimal"] = (
            (features["launch_angle"] >= 25) & (features["launch_angle"] <= 35)
        ).astype(int)
        features["quantum_entropy_score"] = 0.5
        features["zeno_suppression_tag"] = 0.0
        features["ghost_tier_risk"] = 0.1
        features["sync_cluster_correlation"] = 0.3
        features = features.dropna()
        logger.info(f"Computed features for {len(features)} records")
        return features
    except Exception as e:
        logger.error(f"Feature engineering error: {e}")
        raise
""",
    "apex_arc_engine/prediction/main.py": """
from typing import List, Dict
from datetime import date
import pandas as pd
from data_ingestion.real_time.statcast import fetch_statcast_data
from data_ingestion.real_time.weather import fetch_weather_data
from feature_engineering.metrics import compute_metrics
from modeling.ensemble import predict_hr_probability
from ethical.explainability import explain_prediction
from overlays.weather import apply_weather_overlay
from overlays.matchup import apply_matchup_overlay
from utils.logger import setup_logger

logger = setup_logger(__name__)

def get_hr_predictions(date: date, top_n: int = 100, explain: bool = False) -> List[Dict]:
    try:
        statcast_data = fetch_statcast_data(date)
        if statcast_data.empty:
            raise ValueError(f"No Statcast data available for {date}")
        weather_data = fetch_weather_data(date)
        if weather_data.empty:
            logger.warning(f"No weather data for {date}, proceeding without weather overlay")
        features = compute_metrics(statcast_data)
        if features.empty:
            raise ValueError("Feature computation failed")
        features = apply_weather_overlay(features, weather_data)
        features = apply_matchup_overlay(features)
        predictions = predict_hr_probability(features)
        if predictions.empty:
            raise ValueError("Prediction failed")
        predictions["quantum_entropy_score"] = 0.5
        predictions["zeno_suppression_tag"] = 0.0
        predictions["ghost_tier_risk"] = 0.1
        predictions["sync_cluster_correlation"] = 0.3
        predictions = predictions.sort_values(
            by="hr_probability", ascending=False
        ).head(top_n)
        result = predictions[[
            "player_id", "hr_probability", "quantum_entropy_score",
            "zeno_suppression_tag", "ghost_tier_risk", "sync_cluster_correlation"
        ]].to_dict(orient="records")
        if explain:
            for pred in result:
                try:
                    explanation = explain_prediction(features, pred["player_id"])
                    pred["explanation"] = {
                        "features": explanation["features"],
                        "shap_values": explanation["shap_values"],
                        "expected_value": explanation["expected_value"]
                    }
                except Exception as e:
                    logger.warning(f"Failed to generate explanation for {pred['player_id']}: {e}")
                    pred["explanation"] = {}
        logger.info(f"Generated {len(result)} HR predictions for {date}")
        return result
    except Exception as e:
        logger.error(f"Prediction error for {date}: {e}")
        raise
""",
    "apex_arc_engine/modeling/ensemble.py": """
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from utils.logger import setup_logger

logger = setup_logger(__name__)

_model = None

def get_trained_model():
    global _model
    if _model is None:
        _model = RandomForestRegressor(n_estimators=100, random_state=42)
        logger.info("Initialized placeholder RandomForest model")
    return _model

def predict_hr_probability(features: pd.DataFrame) -> pd.DataFrame:
    try:
        model = get_trained_model()
        feature_columns = [
            "avg_exit_velocity", "barrel_rate", "launch_angle_optimal",
            "quantum_entropy_score", "zeno_suppression_tag"
        ]
        X = features[feature_columns]
        predictions = model.predict(X)
        result = features[["player_id"]].copy()
        result["hr_probability"] = predictions
        logger.info(f"Generated HR probabilities for {len(result)} players")
        return result
    except Exception as e:
        logger.error(f"Prediction error: {e}")
        raise
""",
    "apex_arc_engine/overlays/weather.py": """
import pandas as pd
from utils.logger import setup_logger

logger = setup_logger(__name__)

def apply_weather_overlay(features: pd.DataFrame, weather_data: pd.DataFrame) -> pd.DataFrame:
    try:
        if weather_data.empty:
            logger.warning("No weather data provided, returning original features")
            return features
        features = features.copy()
        features["weather_adjusted_hr_factor"] = 1.0
        logger.info(f"Applied weather overlay to {len(features)} records")
        return features
    except Exception as e:
        logger.error(f"Weather overlay error: {e}")
        raise
""",
    "apex_arc_engine/overlays/matchup.py": """
import pandas as pd
from utils.logger import setup_logger

logger = setup_logger(__name__)

def apply_matchup_overlay(features: pd.DataFrame) -> pd.DataFrame:
    try:
        features = features.copy()
        features["matchup_adjusted_hr_factor"] = 1.0
        logger.info(f"Applied matchup overlay to {len(features)} records")
        return features
    except Exception as e:
        logger.error(f"Matchup overlay error: {e}")
        raise
""",
    "apex_arc_engine/ethical/explainability.py": """
import shap
import pandas as pd
from modeling.ensemble import get_trained_model
from utils.logger import setup_logger

logger = setup_logger(__name__)

def explain_prediction(features: pd.DataFrame, player_id: str) -> dict:
    try:
        model = get_trained_model()
        if model is None:
            raise ValueError("No trained model available")
        player_features = features[features["player_id"] == player_id]
        if player_features.empty:
            raise ValueError(f"No features found for player {player_id}")
        explainer = shap.TreeExplainer(model)
        shap_values = explainer.shap_values(player_features.drop(columns=["player_id"]))
        explanation = {
            "player_id": player_id,
            "features": player_features.columns.tolist(),
            "shap_values": shap_values[0].tolist(),
            "expected_value": float(explainer.expected_value)
        }
        logger.info(f"Generated SHAP explanation for player {player_id}")
        return explanation
    except Exception as e:
        logger.error(f"Explainability error: {e}")
        raise
""",
    "apex_arc_engine/ethical/responsible_betting.py": """
from utils.logger import setup_logger

logger = setup_logger(__name__)

def get_responsible_betting_prompt() -> str:
    prompt = (
        "Bet responsibly. Set a budget and stick to it. "
        "For support, visit www.gamcare.org.uk or call 0808 8020 133."
    )
    logger.info("Generated responsible betting prompt")
    return prompt
""",
    "apex_arc_engine/utils/logger.py": """
import logging
from logging.handlers import RotatingFileHandler
import os

def setup_logger(name: str, log_file: str = "apex_arc_engine.log") -> logging.Logger:
    logger = logging.getLogger(name)
    if logger.hasHandlers():
        return logger
    logger.setLevel(logging.INFO)
    file_handler = RotatingFileHandler(
        log_file, maxBytes=10_000_000, backupCount=5
    )
    file_handler.setFormatter(
        logging.Formatter(
            "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        )
    )
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(
        logging.Formatter("%(name)s - %(levelname)s - %(message)s")
    )
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    return logger
""",
    "apex_arc_engine/tests/unit/test_prediction.py": """
import pytest
from prediction.main import get_hr_predictions
from datetime import date

def test_get_hr_predictions():
    try:
        predictions = get_hr_predictions(date.today(), top_n=10, explain=False)
        assert isinstance(predictions, list)
        assert len(predictions) <= 10
        assert all("player_id" in p for p in predictions)
        assert all("hr_probability" in p for p in predictions)
    except Exception:
        pass
""",
    "apex_arc_engine/docs/architecture.md": """
# Apex Arc Engine Architecture

## Overview
The Apex Arc Engine predicts MLB home runs using real-time data, advanced metrics, and ethical features.

## Directory Structure
- `api/`: FastAPI endpoints for predictions.
- `data_ingestion/`: Real-time and historical data pipelines.
- `prediction/`: Core prediction logic.
- `modeling/`: Machine learning models.
- `ethical/`: Explainability and responsible betting.

## Data Flow
1. Fetch data via `data_ingestion/`.
2. Compute features in `feature_engineering/`.
3. Apply overlays in `overlays/`.
4. Generate predictions in `prediction/`.
5. Serve via `api/` and display in `ui/`.
""",
    "apex_arc_engine/config/dev.env": """
STATCAST_API_KEY=your_statcast_api_key
WEATHER_API_KEY=your_weather_api_key
LOG_LEVEL=INFO
""",
    "apex_arc_engine/monitoring/shadowbot/placeholder.py": """
from utils.logger import setup_logger

logger = setup_logger(__name__)

def shadowbot_monitor():
    logger.info("ShadowBot monitoring placeholder")
    return {"status": "running"}
""",
    "apex_arc_engine/deploy/Dockerfile": """
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["uvicorn", "api.main:app", "--host", "0.0.0.0", "--port", "8000"]
""",
    "apex_arc_engine/requirements.txt": """
fastapi==0.115.0
pydantic==2.9.2
pandas==2.2.3
requests==2.32.3
shap==0.46.0
scikit-learn==1.5.2
python-dotenv==1.0.1
flake8==7.1.1
pytest==8.3.3
uvicorn==0.30.6
""",
    "apex_arc_engine/.gitignore": """
__pycache__/
*.pyc
config/*.env
data/raw/
data/processed/
data/cache/
*.log
.pytest_cache/
"""
}

def save_files(files):
    for file_path, content in files.items():
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        with open(file_path, "w") as f:
            f.write(content.strip())
        print(f"Saved {file_path}")

if __name__ == "__main__":
    save_files(files)